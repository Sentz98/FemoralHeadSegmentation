{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpappol\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /tmp/ipykernel_13442/2905192561.py 12 <module>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb Cella 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m Resize, ToTensor, Normalize, Pad, Compose, RandomRotation, RandomHorizontalFlip, RandomVerticalFlip\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Initialize WandB\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m wandb\u001b[39m.\u001b[39;49minit(project\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mFemoral Slice Classifier\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSliceClassifierCNN\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, num_classes, dropout_prob\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/medical/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:1193\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[39massert\u001b[39;00m logger\n\u001b[1;32m   1192\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39minterrupted\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39me)\n\u001b[0;32m-> 1193\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   1194\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1195\u001b[0m     error_seen \u001b[39m=\u001b[39m e\n",
      "File \u001b[0;32m~/anaconda3/envs/medical/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:1170\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1168\u001b[0m except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[1;32m   1169\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1170\u001b[0m     run \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m   1171\u001b[0m     except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[1;32m   1172\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/medical/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:756\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    753\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcommunicating run to backend with \u001b[39m\u001b[39m{\u001b[39;00mtimeout\u001b[39m}\u001b[39;00m\u001b[39m second timeout\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    755\u001b[0m run_init_handle \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39minterface\u001b[39m.\u001b[39mdeliver_run(run)\n\u001b[0;32m--> 756\u001b[0m result \u001b[39m=\u001b[39m run_init_handle\u001b[39m.\u001b[39;49mwait(\n\u001b[1;32m    757\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    758\u001b[0m     on_progress\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_progress_init,\n\u001b[1;32m    759\u001b[0m     cancel\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    760\u001b[0m )\n\u001b[1;32m    761\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    762\u001b[0m     run_result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mrun_result\n",
      "File \u001b[0;32m~/anaconda3/envs/medical/lib/python3.9/site-packages/wandb/sdk/lib/mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interface\u001b[39m.\u001b[39m_transport_keepalive_failed():\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m MailboxError(\u001b[39m\"\u001b[39m\u001b[39mtransport failed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m found, abandoned \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_slot\u001b[39m.\u001b[39;49m_get_and_clear(timeout\u001b[39m=\u001b[39;49mwait_timeout)\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m found:\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Always update progress to 100% when done\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39mif\u001b[39;00m on_progress \u001b[39mand\u001b[39;00m progress_handle \u001b[39mand\u001b[39;00m progress_sent:\n",
      "File \u001b[0;32m~/anaconda3/envs/medical/lib/python3.9/site-packages/wandb/sdk/lib/mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_and_clear\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Optional[pb\u001b[39m.\u001b[39mResult], \u001b[39mbool\u001b[39m]:\n\u001b[1;32m    129\u001b[0m     found \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait(timeout\u001b[39m=\u001b[39;49mtimeout):\n\u001b[1;32m    131\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m             found \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[0;32m~/anaconda3/envs/medical/lib/python3.9/site-packages/wandb/sdk/lib/mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wait\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout\u001b[39m=\u001b[39;49mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/medical/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/envs/medical/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import wandb\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, Pad, Compose, RandomRotation, RandomHorizontalFlip, RandomVerticalFlip\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"Femoral Slice Classifier\")\n",
    "\n",
    "\n",
    "class SliceClassifierCNN(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_prob=0.5):\n",
    "        super(SliceClassifierCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.dropout1 = nn.Dropout2d(p=dropout_prob)  # Add dropout\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.dropout2 = nn.Dropout2d(p=dropout_prob)  # Add dropout\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.dropout3 = nn.Dropout2d(p=dropout_prob)  # Add dropout\n",
    "        self.fc1 = nn.Linear(128 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout1(x)  # Apply dropout\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout2(x)  # Apply dropout\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout3(x)  # Apply dropout\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def train_model(self, dataloader, num_epochs, learning_rate, imbalanceTrue):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f'Using device: {device}')\n",
    "        self.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.tensor([1 - imbalanceTrue, imbalanceTrue]).to(device))\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            # Wrap your dataloader with tqdm for the progress bar\n",
    "            for inputs, labels in tqdm(dataloader, total=len(dataloader), desc=f'Epoch {epoch + 1}/{num_epochs}', colour='red'):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Compute accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            accuracy = correct_predictions / total_samples\n",
    "\n",
    "            wandb.log({\n",
    "                \"Loss\": running_loss / len(dataloader),\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Epoch\": epoch,\n",
    "            })\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(dataloader)}, Accuracy: {accuracy}')\n",
    "\n",
    "        print('Training complete!')\n",
    "\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        print(f'Model saved to {model_path}')\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, model_path, num_classes):\n",
    "        model = cls(num_classes)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        return model    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceClassifierResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SliceClassifierResNet, self).__init__()\n",
    "        # Load a pretrained ResNet-18 model\n",
    "        self.resnet = torchvision.models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Change the first layer to accept 1 channel input\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Freeze all layers except the final classifier layer\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.resnet.fc.requires_grad = True\n",
    "\n",
    "        # Modify the final classifier layer to match the number of classes\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "    def train_model(self, dataloader, num_epochs, learning_rate):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f'Using device: {device}')\n",
    "        self.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            # Wrap your dataloader with tqdm for the progress bar\n",
    "            for inputs, labels in tqdm(dataloader, total=len(dataloader), desc=f'Epoch {epoch + 1}/{num_epochs}', colour='red'):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Compute accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            accuracy = correct_predictions / total_samples\n",
    "\n",
    "            wandb.log({\n",
    "                \"Loss\": running_loss / len(dataloader),\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Epoch\": epoch,\n",
    "            })\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(dataloader)}, Accuracy: {accuracy}')\n",
    "\n",
    "        print('Training complete!')\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        print(f'Model saved to {model_path}')\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, model_path, num_classes):\n",
    "        model = cls(num_classes)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_and_std(dataset):\n",
    "    \"\"\"\n",
    "    Calculate the mean and standard deviation of a dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset (Dataset): a PyTorch Dataset object that returns images.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (mean, std) of the dataset.\n",
    "    \"\"\"\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    total_images = 0\n",
    "\n",
    "    for images, _ in loader:\n",
    "        # Assuming images are stacked in shape (B, C, H, W)\n",
    "        # where B is the batch size, C is the number of channels,\n",
    "        # H is the height and W is the width of the image.\n",
    "\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images += batch_samples\n",
    "\n",
    "    mean /= total_images\n",
    "    std /= total_images\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadToSquare:\n",
    "    def __init__(self, fill):\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert tensor to PIL Image if it's a tensor\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = to_pil_image(img)\n",
    "        \n",
    "        # Calculate padding\n",
    "        width, height = img.size\n",
    "        max_wh = max(width, height)\n",
    "        pad_width = (max_wh - width) // 2\n",
    "        pad_height = (max_wh - height) // 2\n",
    "        padding = (pad_width, pad_height, pad_width if width % 2 == 0 else pad_width + 1, pad_height if height % 2 == 0 else pad_height + 1)\n",
    "        \n",
    "        # Pad and return image\n",
    "        img = Pad(padding, fill=self.fill)(img)\n",
    "        return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SliceDatasetBuilder import CustomHipDataset\n",
    "import datetime\n",
    "\n",
    "json_path = \"label.json\"  # Update this to the path where your image dataset is located\n",
    "num_classes = 2\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "date = datetime.datetime.now().strftime(\"%H%M%S\")\n",
    "model_path = f\"modelCNN_{date}.pth\"\n",
    "\n",
    "mean = 0.2143\n",
    "std = 0.1621\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/dataset/normalHip/JOR01/axial...\n",
      "Positive interval of JOR01: [330, 400] for SX\n",
      "Positive interval of JOR01: [280, 370] for DX\n",
      "Loading data/dataset/normalHip/JOR02/axial...\n",
      "Positive interval of JOR02: [250, 380] for SX\n",
      "Positive interval of JOR02: [340, 445] for DX\n",
      "Loading data/dataset/normalHip/JOR09/axial...\n",
      "Positive interval of JOR09: [240, 300] for SX\n",
      "Positive interval of JOR09: [230, 290] for DX\n",
      "Loading data/dataset/dysplasticHip/TRAD09/axial...\n",
      "Positive interval of TRAD09: [130, 190] for SX\n",
      "Positive interval of TRAD09: [255, 320] for DX\n",
      "Loading data/dataset/dysplasticHip/TRAD10/axial...\n",
      "Positive interval of TRAD10: [355, 430] for SX\n",
      "Positive interval of TRAD10: [230, 325] for DX\n",
      "Loading data/dataset/retrovertedHip/Patient_01/axial...\n",
      "Positive interval of Patient_01: [210, 270] for SX\n",
      "Positive interval of Patient_01: [200, 260] for DX\n",
      "Loading data/dataset/retrovertedHip/Patient_02/axial...\n",
      "Positive interval of Patient_02: [230, 285] for SX\n",
      "Positive interval of Patient_02: [260, 310] for DX\n",
      "Total number of samples: 9824\n",
      "Number of positive samples: 903\n",
      "Number of negative samples: 8921\n",
      "Imbalance ratio: 0.09\n"
     ]
    }
   ],
   "source": [
    "transform = Compose([\n",
    "    RandomRotation(degrees=(-10, 10)),  # Randomly rotate the image within the range of -10 to 10 degrees\n",
    "    RandomHorizontalFlip(0.5),  # Randomly flip the image horizontally\n",
    "    RandomVerticalFlip(0.5),  # Randomly flip the image vertically\n",
    "    PadToSquare(fill=mean),  # Pad the image to make it square\n",
    "    Resize((512, 512)),  # Resize the image to the desired size\n",
    "    ToTensor(),\n",
    "    Normalize(mean, std)  # Normalize with the dataset's mean and std\n",
    "])\n",
    "\n",
    "dataset = CustomHipDataset(json_path,['axial'], transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Plot some info about the dataset\n",
    "print(f'Total number of samples: {len(dataset)}')\n",
    "image_paths, labels = dataset.get_all_dataset()\n",
    "print(f'Number of positive samples: {sum(labels)}')\n",
    "print(f'Number of negative samples: {len(labels) - sum(labels)}')\n",
    "\n",
    "imbalanceTrue = round(sum(labels) / len(labels), 2)\n",
    "print(f'Imbalance ratio: {imbalanceTrue}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|\u001b[31m██████████\u001b[0m| 307/307 [03:52<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 2.0359769478438485, Accuracy: 0.9006514657980456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|\u001b[31m██████████\u001b[0m| 307/307 [03:39<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.12186172155736177, Accuracy: 0.9071661237785016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|\u001b[31m██████████\u001b[0m| 307/307 [03:44<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.06570267538343448, Accuracy: 0.9080822475570033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|\u001b[31m██████████\u001b[0m| 307/307 [03:46<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.05426401501170326, Accuracy: 0.9097109120521173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|\u001b[31m██████████\u001b[0m| 307/307 [03:47<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.047537715978926524, Accuracy: 0.9125610749185668\n",
      "Training complete!\n",
      "Model saved to modelCNN_172058.pth\n"
     ]
    }
   ],
   "source": [
    "# Select model\n",
    "TRAIN = True\n",
    "\n",
    "if TRAIN:\n",
    "    #model = SliceClassifierResNet(num_classes)\n",
    "    model = SliceClassifierCNN(num_classes)\n",
    "    # Train the model \n",
    "    model.train_model(dataloader, num_epochs, learning_rate, imbalanceTrue)\n",
    "    model.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SliceClassifierCNN:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"conv2.weight\", \"conv2.bias\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"conv3.weight\", \"conv3.bias\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"resnet.conv1.weight\", \"resnet.bn1.weight\", \"resnet.bn1.bias\", \"resnet.bn1.running_mean\", \"resnet.bn1.running_var\", \"resnet.bn1.num_batches_tracked\", \"resnet.layer1.0.conv1.weight\", \"resnet.layer1.0.bn1.weight\", \"resnet.layer1.0.bn1.bias\", \"resnet.layer1.0.bn1.running_mean\", \"resnet.layer1.0.bn1.running_var\", \"resnet.layer1.0.bn1.num_batches_tracked\", \"resnet.layer1.0.conv2.weight\", \"resnet.layer1.0.bn2.weight\", \"resnet.layer1.0.bn2.bias\", \"resnet.layer1.0.bn2.running_mean\", \"resnet.layer1.0.bn2.running_var\", \"resnet.layer1.0.bn2.num_batches_tracked\", \"resnet.layer1.1.conv1.weight\", \"resnet.layer1.1.bn1.weight\", \"resnet.layer1.1.bn1.bias\", \"resnet.layer1.1.bn1.running_mean\", \"resnet.layer1.1.bn1.running_var\", \"resnet.layer1.1.bn1.num_batches_tracked\", \"resnet.layer1.1.conv2.weight\", \"resnet.layer1.1.bn2.weight\", \"resnet.layer1.1.bn2.bias\", \"resnet.layer1.1.bn2.running_mean\", \"resnet.layer1.1.bn2.running_var\", \"resnet.layer1.1.bn2.num_batches_tracked\", \"resnet.layer2.0.conv1.weight\", \"resnet.layer2.0.bn1.weight\", \"resnet.layer2.0.bn1.bias\", \"resnet.layer2.0.bn1.running_mean\", \"resnet.layer2.0.bn1.running_var\", \"resnet.layer2.0.bn1.num_batches_tracked\", \"resnet.layer2.0.conv2.weight\", \"resnet.layer2.0.bn2.weight\", \"resnet.layer2.0.bn2.bias\", \"resnet.layer2.0.bn2.running_mean\", \"resnet.layer2.0.bn2.running_var\", \"resnet.layer2.0.bn2.num_batches_tracked\", \"resnet.layer2.0.downsample.0.weight\", \"resnet.layer2.0.downsample.1.weight\", \"resnet.layer2.0.downsample.1.bias\", \"resnet.layer2.0.downsample.1.running_mean\", \"resnet.layer2.0.downsample.1.running_var\", \"resnet.layer2.0.downsample.1.num_batches_tracked\", \"resnet.layer2.1.conv1.weight\", \"resnet.layer2.1.bn1.weight\", \"resnet.layer2.1.bn1.bias\", \"resnet.layer2.1.bn1.running_mean\", \"resnet.layer2.1.bn1.running_var\", \"resnet.layer2.1.bn1.num_batches_tracked\", \"resnet.layer2.1.conv2.weight\", \"resnet.layer2.1.bn2.weight\", \"resnet.layer2.1.bn2.bias\", \"resnet.layer2.1.bn2.running_mean\", \"resnet.layer2.1.bn2.running_var\", \"resnet.layer2.1.bn2.num_batches_tracked\", \"resnet.layer3.0.conv1.weight\", \"resnet.layer3.0.bn1.weight\", \"resnet.layer3.0.bn1.bias\", \"resnet.layer3.0.bn1.running_mean\", \"resnet.layer3.0.bn1.running_var\", \"resnet.layer3.0.bn1.num_batches_tracked\", \"resnet.layer3.0.conv2.weight\", \"resnet.layer3.0.bn2.weight\", \"resnet.layer3.0.bn2.bias\", \"resnet.layer3.0.bn2.running_mean\", \"resnet.layer3.0.bn2.running_var\", \"resnet.layer3.0.bn2.num_batches_tracked\", \"resnet.layer3.0.downsample.0.weight\", \"resnet.layer3.0.downsample.1.weight\", \"resnet.layer3.0.downsample.1.bias\", \"resnet.layer3.0.downsample.1.running_mean\", \"resnet.layer3.0.downsample.1.running_var\", \"resnet.layer3.0.downsample.1.num_batches_tracked\", \"resnet.layer3.1.conv1.weight\", \"resnet.layer3.1.bn1.weight\", \"resnet.layer3.1.bn1.bias\", \"resnet.layer3.1.bn1.running_mean\", \"resnet.layer3.1.bn1.running_var\", \"resnet.layer3.1.bn1.num_batches_tracked\", \"resnet.layer3.1.conv2.weight\", \"resnet.layer3.1.bn2.weight\", \"resnet.layer3.1.bn2.bias\", \"resnet.layer3.1.bn2.running_mean\", \"resnet.layer3.1.bn2.running_var\", \"resnet.layer3.1.bn2.num_batches_tracked\", \"resnet.layer4.0.conv1.weight\", \"resnet.layer4.0.bn1.weight\", \"resnet.layer4.0.bn1.bias\", \"resnet.layer4.0.bn1.running_mean\", \"resnet.layer4.0.bn1.running_var\", \"resnet.layer4.0.bn1.num_batches_tracked\", \"resnet.layer4.0.conv2.weight\", \"resnet.layer4.0.bn2.weight\", \"resnet.layer4.0.bn2.bias\", \"resnet.layer4.0.bn2.running_mean\", \"resnet.layer4.0.bn2.running_var\", \"resnet.layer4.0.bn2.num_batches_tracked\", \"resnet.layer4.0.downsample.0.weight\", \"resnet.layer4.0.downsample.1.weight\", \"resnet.layer4.0.downsample.1.bias\", \"resnet.layer4.0.downsample.1.running_mean\", \"resnet.layer4.0.downsample.1.running_var\", \"resnet.layer4.0.downsample.1.num_batches_tracked\", \"resnet.layer4.1.conv1.weight\", \"resnet.layer4.1.bn1.weight\", \"resnet.layer4.1.bn1.bias\", \"resnet.layer4.1.bn1.running_mean\", \"resnet.layer4.1.bn1.running_var\", \"resnet.layer4.1.bn1.num_batches_tracked\", \"resnet.layer4.1.conv2.weight\", \"resnet.layer4.1.bn2.weight\", \"resnet.layer4.1.bn2.bias\", \"resnet.layer4.1.bn2.running_mean\", \"resnet.layer4.1.bn2.running_var\", \"resnet.layer4.1.bn2.num_batches_tracked\", \"resnet.fc.weight\", \"resnet.fc.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb Cella 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#TEST CLASSIFIER\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m loaded \u001b[39m=\u001b[39m SliceClassifierCNN\u001b[39m.\u001b[39;49mload_model(\u001b[39m'\u001b[39;49m\u001b[39mmodelCNN_172058.pth\u001b[39;49m\u001b[39m'\u001b[39;49m, num_classes)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m loaded\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Load images from a folder\u001b[39;00m\n",
      "\u001b[1;32m/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb Cella 8\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(\u001b[39mcls\u001b[39m, model_path, num_classes):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(num_classes)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m     model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(model_path))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/medical/lib/python3.9/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SliceClassifierCNN:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"conv2.weight\", \"conv2.bias\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"conv3.weight\", \"conv3.bias\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"resnet.conv1.weight\", \"resnet.bn1.weight\", \"resnet.bn1.bias\", \"resnet.bn1.running_mean\", \"resnet.bn1.running_var\", \"resnet.bn1.num_batches_tracked\", \"resnet.layer1.0.conv1.weight\", \"resnet.layer1.0.bn1.weight\", \"resnet.layer1.0.bn1.bias\", \"resnet.layer1.0.bn1.running_mean\", \"resnet.layer1.0.bn1.running_var\", \"resnet.layer1.0.bn1.num_batches_tracked\", \"resnet.layer1.0.conv2.weight\", \"resnet.layer1.0.bn2.weight\", \"resnet.layer1.0.bn2.bias\", \"resnet.layer1.0.bn2.running_mean\", \"resnet.layer1.0.bn2.running_var\", \"resnet.layer1.0.bn2.num_batches_tracked\", \"resnet.layer1.1.conv1.weight\", \"resnet.layer1.1.bn1.weight\", \"resnet.layer1.1.bn1.bias\", \"resnet.layer1.1.bn1.running_mean\", \"resnet.layer1.1.bn1.running_var\", \"resnet.layer1.1.bn1.num_batches_tracked\", \"resnet.layer1.1.conv2.weight\", \"resnet.layer1.1.bn2.weight\", \"resnet.layer1.1.bn2.bias\", \"resnet.layer1.1.bn2.running_mean\", \"resnet.layer1.1.bn2.running_var\", \"resnet.layer1.1.bn2.num_batches_tracked\", \"resnet.layer2.0.conv1.weight\", \"resnet.layer2.0.bn1.weight\", \"resnet.layer2.0.bn1.bias\", \"resnet.layer2.0.bn1.running_mean\", \"resnet.layer2.0.bn1.running_var\", \"resnet.layer2.0.bn1.num_batches_tracked\", \"resnet.layer2.0.conv2.weight\", \"resnet.layer2.0.bn2.weight\", \"resnet.layer2.0.bn2.bias\", \"resnet.layer2.0.bn2.running_mean\", \"resnet.layer2.0.bn2.running_var\", \"resnet.layer2.0.bn2.num_batches_tracked\", \"resnet.layer2.0.downsample.0.weight\", \"resnet.layer2.0.downsample.1.weight\", \"resnet.layer2.0.downsample.1.bias\", \"resnet.layer2.0.downsample.1.running_mean\", \"resnet.layer2.0.downsample.1.running_var\", \"resnet.layer2.0.downsample.1.num_batches_tracked\", \"resnet.layer2.1.conv1.weight\", \"resnet.layer2.1.bn1.weight\", \"resnet.layer2.1.bn1.bias\", \"resnet.layer2.1.bn1.running_mean\", \"resnet.layer2.1.bn1.running_var\", \"resnet.layer2.1.bn1.num_batches_tracked\", \"resnet.layer2.1.conv2.weight\", \"resnet.layer2.1.bn2.weight\", \"resnet.layer2.1.bn2.bias\", \"resnet.layer2.1.bn2.running_mean\", \"resnet.layer2.1.bn2.running_var\", \"resnet.layer2.1.bn2.num_batches_tracked\", \"resnet.layer3.0.conv1.weight\", \"resnet.layer3.0.bn1.weight\", \"resnet.layer3.0.bn1.bias\", \"resnet.layer3.0.bn1.running_mean\", \"resnet.layer3.0.bn1.running_var\", \"resnet.layer3.0.bn1.num_batches_tracked\", \"resnet.layer3.0.conv2.weight\", \"resnet.layer3.0.bn2.weight\", \"resnet.layer3.0.bn2.bias\", \"resnet.layer3.0.bn2.running_mean\", \"resnet.layer3.0.bn2.running_var\", \"resnet.layer3.0.bn2.num_batches_tracked\", \"resnet.layer3.0.downsample.0.weight\", \"resnet.layer3.0.downsample.1.weight\", \"resnet.layer3.0.downsample.1.bias\", \"resnet.layer3.0.downsample.1.running_mean\", \"resnet.layer3.0.downsample.1.running_var\", \"resnet.layer3.0.downsample.1.num_batches_tracked\", \"resnet.layer3.1.conv1.weight\", \"resnet.layer3.1.bn1.weight\", \"resnet.layer3.1.bn1.bias\", \"resnet.layer3.1.bn1.running_mean\", \"resnet.layer3.1.bn1.running_var\", \"resnet.layer3.1.bn1.num_batches_tracked\", \"resnet.layer3.1.conv2.weight\", \"resnet.layer3.1.bn2.weight\", \"resnet.layer3.1.bn2.bias\", \"resnet.layer3.1.bn2.running_mean\", \"resnet.layer3.1.bn2.running_var\", \"resnet.layer3.1.bn2.num_batches_tracked\", \"resnet.layer4.0.conv1.weight\", \"resnet.layer4.0.bn1.weight\", \"resnet.layer4.0.bn1.bias\", \"resnet.layer4.0.bn1.running_mean\", \"resnet.layer4.0.bn1.running_var\", \"resnet.layer4.0.bn1.num_batches_tracked\", \"resnet.layer4.0.conv2.weight\", \"resnet.layer4.0.bn2.weight\", \"resnet.layer4.0.bn2.bias\", \"resnet.layer4.0.bn2.running_mean\", \"resnet.layer4.0.bn2.running_var\", \"resnet.layer4.0.bn2.num_batches_tracked\", \"resnet.layer4.0.downsample.0.weight\", \"resnet.layer4.0.downsample.1.weight\", \"resnet.layer4.0.downsample.1.bias\", \"resnet.layer4.0.downsample.1.running_mean\", \"resnet.layer4.0.downsample.1.running_var\", \"resnet.layer4.0.downsample.1.num_batches_tracked\", \"resnet.layer4.1.conv1.weight\", \"resnet.layer4.1.bn1.weight\", \"resnet.layer4.1.bn1.bias\", \"resnet.layer4.1.bn1.running_mean\", \"resnet.layer4.1.bn1.running_var\", \"resnet.layer4.1.bn1.num_batches_tracked\", \"resnet.layer4.1.conv2.weight\", \"resnet.layer4.1.bn2.weight\", \"resnet.layer4.1.bn2.bias\", \"resnet.layer4.1.bn2.running_mean\", \"resnet.layer4.1.bn2.running_var\", \"resnet.layer4.1.bn2.num_batches_tracked\", \"resnet.fc.weight\", \"resnet.fc.bias\". "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "#TEST CLASSIFIER\n",
    "loaded = SliceClassifierCNN.load_model('modelCNN_172058.pth', num_classes)\n",
    "loaded.eval()\n",
    "\n",
    "# Load images from a folder\n",
    "folder_path = \"data/dataset/retrovertedHip/Patient_06/axial\"\n",
    "\n",
    "image_array = []\n",
    "for filename in os.listdir(folder_path):\n",
    "        # Sample every 10 images\n",
    "        if int(filename.split('_')[-2]) % 20 != 0:\n",
    "                continue\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(file_path).convert(\"RGB\").convert(\"L\")\n",
    "        if transform is not None:\n",
    "                image = transform(image)\n",
    "        image_array.append(image)\n",
    "\n",
    "print(f'Number of images: {len(image_array)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the loaded model\n",
    "predictions = []\n",
    "for image in image_array:\n",
    "    image = image.unsqueeze(0)\n",
    "    predictions.append(loaded(image))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the predictions\n",
    "for index, value in enumerate(predictions):\n",
    "    # Get the predicted class with the highest score\n",
    "    pred_class = value.argmax(dim=1, keepdim=True)\n",
    "    if pred_class.item() == 1:\n",
    "        print(f'{index + 1}: Normal')\n",
    "        plt.imshow(image_array[index].squeeze(), cmap='gray')\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|\u001b[31m██████████\u001b[0m| 307/307 [03:22<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.28315032223779524, Accuracy: 0.9077768729641694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|\u001b[31m██████████\u001b[0m| 307/307 [03:31<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.25588731103688966, Accuracy: 0.9084894136807817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|\u001b[31m██████████\u001b[0m| 307/307 [03:36<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.2333352639421772, Accuracy: 0.9113395765472313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|\u001b[31m██████████\u001b[0m| 307/307 [03:23<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.22448269366392873, Accuracy: 0.9120521172638436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|\u001b[31m██████████\u001b[0m| 307/307 [03:30<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.21313238762780198, Accuracy: 0.9164291530944625\n",
      "Training complete!\n",
      "Model saved to modelCNN_172058.pth\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    model = SliceClassifierResNet(num_classes)\n",
    "    #model = SliceClassifierCNN(num_classes)\n",
    "    # Train the model \n",
    "    model.train_model(dataloader, num_epochs, learning_rate)\n",
    "    model.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 100\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "#TEST CLASSIFIER\n",
    "loaded = SliceClassifierResNet.load_model('modelCNN_172058.pth', num_classes)\n",
    "\n",
    "# Load images from a folder\n",
    "folder_path = \"data/dataset/retrovertedHip/Patient_06/axial\"\n",
    "\n",
    "image_array = []\n",
    "for filename in os.listdir(folder_path):\n",
    "        # Sample every 10 images\n",
    "        if int(filename.split('_')[-2]) % 20 != 0:\n",
    "                continue\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(file_path).convert(\"RGB\").convert(\"L\")\n",
    "        if transform is not None:\n",
    "                image = transform(image)\n",
    "        image_array.append(image)\n",
    "\n",
    "print(f'Number of images: {len(image_array)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the loaded model\n",
    "model.cpu()\n",
    "predictions = []\n",
    "for image in image_array:\n",
    "    image = image.unsqueeze(0)\n",
    "    predictions.append(model(image))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the predictions\n",
    "for index, value in enumerate(predictions):\n",
    "    # Get the predicted class with the highest score\n",
    "    pred_class = value.argmax(dim=1, keepdim=True)\n",
    "    if pred_class.item() == 1:\n",
    "        print(f'{index + 1}: Normal')\n",
    "        plt.imshow(image_array[index].squeeze(), cmap='gray')\n",
    "        plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medImg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
