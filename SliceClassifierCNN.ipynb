{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpappol\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/wandb/run-20231103_222324-tf4nfsug</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pappol/Femoral%20Slice%20Classifier/runs/tf4nfsug' target=\"_blank\">expert-paper-46</a></strong> to <a href='https://wandb.ai/pappol/Femoral%20Slice%20Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pappol/Femoral%20Slice%20Classifier' target=\"_blank\">https://wandb.ai/pappol/Femoral%20Slice%20Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pappol/Femoral%20Slice%20Classifier/runs/tf4nfsug' target=\"_blank\">https://wandb.ai/pappol/Femoral%20Slice%20Classifier/runs/tf4nfsug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import wandb\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, Pad, Compose, RandomRotation, RandomHorizontalFlip, RandomVerticalFlip\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"Femoral Slice Classifier\")\n",
    "\n",
    "\n",
    "class SliceClassifierCNN(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_prob=0.5):\n",
    "        super(SliceClassifierCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.dropout1 = nn.Dropout2d(p=dropout_prob)  # Add dropout\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.dropout2 = nn.Dropout2d(p=dropout_prob)  # Add dropout\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.dropout3 = nn.Dropout2d(p=dropout_prob)  # Add dropout\n",
    "        self.fc1 = nn.Linear(128 * 64 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout1(x)  # Apply dropout\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout2(x)  # Apply dropout\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout3(x)  # Apply dropout\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def train_model(self, dataloader, num_epochs, learning_rate, imbalanceTrue):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f'Using device: {device}')\n",
    "        self.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.tensor([1 - imbalanceTrue, imbalanceTrue]).to(device))\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            # Wrap your dataloader with tqdm for the progress bar\n",
    "            for inputs, labels in tqdm(dataloader, total=len(dataloader), desc=f'Epoch {epoch + 1}/{num_epochs}', colour='red'):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Compute accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            accuracy = correct_predictions / total_samples\n",
    "\n",
    "            wandb.log({\n",
    "                \"Loss\": running_loss / len(dataloader),\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Epoch\": epoch,\n",
    "            })\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(dataloader)}, Accuracy: {accuracy}')\n",
    "\n",
    "        print('Training complete!')\n",
    "\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        print(f'Model saved to {model_path}')\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, model_path, num_classes):\n",
    "        model = cls(num_classes)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        return model    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceClassifierResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SliceClassifierResNet, self).__init__()\n",
    "        # Load a pretrained ResNet-18 model\n",
    "        self.resnet = torchvision.models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Change the first layer to accept 1 channel input\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Freeze all layers except the final classifier layer\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.resnet.fc.requires_grad = True\n",
    "\n",
    "        # Modify the final classifier layer to match the number of classes\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "    def train_model(self, dataloader, num_epochs, learning_rate):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f'Using device: {device}')\n",
    "        self.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            # Wrap your dataloader with tqdm for the progress bar\n",
    "            for inputs, labels in tqdm(dataloader, total=len(dataloader), desc=f'Epoch {epoch + 1}/{num_epochs}', colour='red'):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Compute accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            accuracy = correct_predictions / total_samples\n",
    "\n",
    "            wandb.log({\n",
    "                \"Loss\": running_loss / len(dataloader),\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Epoch\": epoch,\n",
    "            })\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(dataloader)}, Accuracy: {accuracy}')\n",
    "\n",
    "        print('Training complete!')\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        print(f'Model saved to {model_path}')\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, model_path, num_classes):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = cls(num_classes)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_and_std(dataset):\n",
    "    \"\"\"\n",
    "    Calculate the mean and standard deviation of a dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset (Dataset): a PyTorch Dataset object that returns images.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (mean, std) of the dataset.\n",
    "    \"\"\"\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    total_images = 0\n",
    "\n",
    "    for images, _ in loader:\n",
    "        # Assuming images are stacked in shape (B, C, H, W)\n",
    "        # where B is the batch size, C is the number of channels,\n",
    "        # H is the height and W is the width of the image.\n",
    "\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images += batch_samples\n",
    "\n",
    "    mean /= total_images\n",
    "    std /= total_images\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadToSquare:\n",
    "    def __init__(self, fill):\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert tensor to PIL Image if it's a tensor\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = to_pil_image(img)\n",
    "        \n",
    "        # Calculate padding\n",
    "        width, height = img.size\n",
    "        max_wh = max(width, height)\n",
    "        pad_width = (max_wh - width) // 2\n",
    "        pad_height = (max_wh - height) // 2\n",
    "        padding = (pad_width, pad_height, pad_width if width % 2 == 0 else pad_width + 1, pad_height if height % 2 == 0 else pad_height + 1)\n",
    "        \n",
    "        # Pad and return image\n",
    "        img = Pad(padding, fill=self.fill)(img)\n",
    "        return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SliceDatasetBuilder import CustomHipDataset\n",
    "import datetime\n",
    "\n",
    "json_path = \"label.json\"  # Update this to the path where your image dataset is located\n",
    "num_classes = 2\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "date = datetime.datetime.now().strftime(\"%H%M%S\")\n",
    "model_path = f\"modelCNN_{date}.pth\"\n",
    "\n",
    "mean = 0.2143\n",
    "std = 0.1621\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/dataset/normalHip/JOR01/axial...\n",
      "Positive interval of JOR01: [330, 400] for SX\n",
      "Positive interval of JOR01: [280, 370] for DX\n",
      "Loading data/dataset/normalHip/JOR02/axial...\n",
      "Positive interval of JOR02: [250, 380] for SX\n",
      "Positive interval of JOR02: [340, 445] for DX\n",
      "Loading data/dataset/normalHip/JOR09/axial...\n",
      "Positive interval of JOR09: [240, 300] for SX\n",
      "Positive interval of JOR09: [230, 290] for DX\n",
      "Loading data/dataset/dysplasticHip/TRAD09/axial...\n",
      "Positive interval of TRAD09: [130, 190] for SX\n",
      "Positive interval of TRAD09: [255, 320] for DX\n",
      "Loading data/dataset/dysplasticHip/TRAD10/axial...\n",
      "Positive interval of TRAD10: [355, 430] for SX\n",
      "Positive interval of TRAD10: [230, 325] for DX\n",
      "Loading data/dataset/retrovertedHip/Patient_01/axial...\n",
      "Positive interval of Patient_01: [210, 270] for SX\n",
      "Positive interval of Patient_01: [200, 260] for DX\n",
      "Loading data/dataset/retrovertedHip/Patient_02/axial...\n",
      "Positive interval of Patient_02: [230, 285] for SX\n",
      "Positive interval of Patient_02: [260, 310] for DX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Balancing dataset: 100%|\u001b[35m██████████\u001b[0m| 9824/9824 [00:00<00:00, 3355443.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset balanced: 903 positive images and 903 negative images\n",
      "Total number of samples: 1806\n",
      "Number of positive samples: 903\n",
      "Number of negative samples: 903\n",
      "Imbalance ratio: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = Compose([\n",
    "    RandomRotation(degrees=(-10, 10)),  # Randomly rotate the image within the range of -10 to 10 degrees\n",
    "    RandomHorizontalFlip(0.5),  # Randomly flip the image horizontally\n",
    "    RandomVerticalFlip(0.5),  # Randomly flip the image vertically\n",
    "    PadToSquare(fill=mean),  # Pad the image to make it square\n",
    "    Resize((512, 512)),  # Resize the image to the desired size\n",
    "    ToTensor(),\n",
    "    Normalize(mean, std)  # Normalize with the dataset's mean and std\n",
    "])\n",
    "\n",
    "dataset = CustomHipDataset(json_path,['axial'], transform=transform)\n",
    "dataset.balance_dataset()\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Plot some info about the dataset\n",
    "print(f'Total number of samples: {len(dataset)}')\n",
    "image_paths, labels = dataset.get_all_dataset()\n",
    "print(f'Number of positive samples: {sum(labels)}')\n",
    "print(f'Number of negative samples: {len(labels) - sum(labels)}')\n",
    "\n",
    "imbalanceTrue = round(sum(labels) / len(labels), 2)\n",
    "print(f'Imbalance ratio: {imbalanceTrue}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|\u001b[31m          \u001b[0m| 0/57 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 7.79 GiB total capacity; 1.28 GiB already allocated; 658.44 MiB free; 1.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m SliceClassifierCNN(num_classes)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Train the model \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain_model(dataloader, num_epochs, learning_rate, imbalanceTrue)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m model\u001b[39m.\u001b[39msave_model(model_path)\n",
      "\u001b[1;32m/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/medical/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)  \u001b[39m# Apply dropout\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/medical/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/medical/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/medical/lib/python3.9/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 7.79 GiB total capacity; 1.28 GiB already allocated; 658.44 MiB free; 1.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Select model\n",
    "TRAIN = True\n",
    "\n",
    "if TRAIN:\n",
    "    #model = SliceClassifierResNet(num_classes)\n",
    "    model = SliceClassifierCNN(num_classes)\n",
    "    # Train the model \n",
    "    model.train_model(dataloader, num_epochs, learning_rate, imbalanceTrue)\n",
    "    model.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SliceClassifierCNN:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"conv2.weight\", \"conv2.bias\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"conv3.weight\", \"conv3.bias\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"resnet.conv1.weight\", \"resnet.bn1.weight\", \"resnet.bn1.bias\", \"resnet.bn1.running_mean\", \"resnet.bn1.running_var\", \"resnet.bn1.num_batches_tracked\", \"resnet.layer1.0.conv1.weight\", \"resnet.layer1.0.bn1.weight\", \"resnet.layer1.0.bn1.bias\", \"resnet.layer1.0.bn1.running_mean\", \"resnet.layer1.0.bn1.running_var\", \"resnet.layer1.0.bn1.num_batches_tracked\", \"resnet.layer1.0.conv2.weight\", \"resnet.layer1.0.bn2.weight\", \"resnet.layer1.0.bn2.bias\", \"resnet.layer1.0.bn2.running_mean\", \"resnet.layer1.0.bn2.running_var\", \"resnet.layer1.0.bn2.num_batches_tracked\", \"resnet.layer1.1.conv1.weight\", \"resnet.layer1.1.bn1.weight\", \"resnet.layer1.1.bn1.bias\", \"resnet.layer1.1.bn1.running_mean\", \"resnet.layer1.1.bn1.running_var\", \"resnet.layer1.1.bn1.num_batches_tracked\", \"resnet.layer1.1.conv2.weight\", \"resnet.layer1.1.bn2.weight\", \"resnet.layer1.1.bn2.bias\", \"resnet.layer1.1.bn2.running_mean\", \"resnet.layer1.1.bn2.running_var\", \"resnet.layer1.1.bn2.num_batches_tracked\", \"resnet.layer2.0.conv1.weight\", \"resnet.layer2.0.bn1.weight\", \"resnet.layer2.0.bn1.bias\", \"resnet.layer2.0.bn1.running_mean\", \"resnet.layer2.0.bn1.running_var\", \"resnet.layer2.0.bn1.num_batches_tracked\", \"resnet.layer2.0.conv2.weight\", \"resnet.layer2.0.bn2.weight\", \"resnet.layer2.0.bn2.bias\", \"resnet.layer2.0.bn2.running_mean\", \"resnet.layer2.0.bn2.running_var\", \"resnet.layer2.0.bn2.num_batches_tracked\", \"resnet.layer2.0.downsample.0.weight\", \"resnet.layer2.0.downsample.1.weight\", \"resnet.layer2.0.downsample.1.bias\", \"resnet.layer2.0.downsample.1.running_mean\", \"resnet.layer2.0.downsample.1.running_var\", \"resnet.layer2.0.downsample.1.num_batches_tracked\", \"resnet.layer2.1.conv1.weight\", \"resnet.layer2.1.bn1.weight\", \"resnet.layer2.1.bn1.bias\", \"resnet.layer2.1.bn1.running_mean\", \"resnet.layer2.1.bn1.running_var\", \"resnet.layer2.1.bn1.num_batches_tracked\", \"resnet.layer2.1.conv2.weight\", \"resnet.layer2.1.bn2.weight\", \"resnet.layer2.1.bn2.bias\", \"resnet.layer2.1.bn2.running_mean\", \"resnet.layer2.1.bn2.running_var\", \"resnet.layer2.1.bn2.num_batches_tracked\", \"resnet.layer3.0.conv1.weight\", \"resnet.layer3.0.bn1.weight\", \"resnet.layer3.0.bn1.bias\", \"resnet.layer3.0.bn1.running_mean\", \"resnet.layer3.0.bn1.running_var\", \"resnet.layer3.0.bn1.num_batches_tracked\", \"resnet.layer3.0.conv2.weight\", \"resnet.layer3.0.bn2.weight\", \"resnet.layer3.0.bn2.bias\", \"resnet.layer3.0.bn2.running_mean\", \"resnet.layer3.0.bn2.running_var\", \"resnet.layer3.0.bn2.num_batches_tracked\", \"resnet.layer3.0.downsample.0.weight\", \"resnet.layer3.0.downsample.1.weight\", \"resnet.layer3.0.downsample.1.bias\", \"resnet.layer3.0.downsample.1.running_mean\", \"resnet.layer3.0.downsample.1.running_var\", \"resnet.layer3.0.downsample.1.num_batches_tracked\", \"resnet.layer3.1.conv1.weight\", \"resnet.layer3.1.bn1.weight\", \"resnet.layer3.1.bn1.bias\", \"resnet.layer3.1.bn1.running_mean\", \"resnet.layer3.1.bn1.running_var\", \"resnet.layer3.1.bn1.num_batches_tracked\", \"resnet.layer3.1.conv2.weight\", \"resnet.layer3.1.bn2.weight\", \"resnet.layer3.1.bn2.bias\", \"resnet.layer3.1.bn2.running_mean\", \"resnet.layer3.1.bn2.running_var\", \"resnet.layer3.1.bn2.num_batches_tracked\", \"resnet.layer4.0.conv1.weight\", \"resnet.layer4.0.bn1.weight\", \"resnet.layer4.0.bn1.bias\", \"resnet.layer4.0.bn1.running_mean\", \"resnet.layer4.0.bn1.running_var\", \"resnet.layer4.0.bn1.num_batches_tracked\", \"resnet.layer4.0.conv2.weight\", \"resnet.layer4.0.bn2.weight\", \"resnet.layer4.0.bn2.bias\", \"resnet.layer4.0.bn2.running_mean\", \"resnet.layer4.0.bn2.running_var\", \"resnet.layer4.0.bn2.num_batches_tracked\", \"resnet.layer4.0.downsample.0.weight\", \"resnet.layer4.0.downsample.1.weight\", \"resnet.layer4.0.downsample.1.bias\", \"resnet.layer4.0.downsample.1.running_mean\", \"resnet.layer4.0.downsample.1.running_var\", \"resnet.layer4.0.downsample.1.num_batches_tracked\", \"resnet.layer4.1.conv1.weight\", \"resnet.layer4.1.bn1.weight\", \"resnet.layer4.1.bn1.bias\", \"resnet.layer4.1.bn1.running_mean\", \"resnet.layer4.1.bn1.running_var\", \"resnet.layer4.1.bn1.num_batches_tracked\", \"resnet.layer4.1.conv2.weight\", \"resnet.layer4.1.bn2.weight\", \"resnet.layer4.1.bn2.bias\", \"resnet.layer4.1.bn2.running_mean\", \"resnet.layer4.1.bn2.running_var\", \"resnet.layer4.1.bn2.num_batches_tracked\", \"resnet.fc.weight\", \"resnet.fc.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#TEST CLASSIFIER\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m loaded \u001b[39m=\u001b[39m SliceClassifierCNN\u001b[39m.\u001b[39;49mload_model(\u001b[39m'\u001b[39;49m\u001b[39mmodelCNN_172058.pth\u001b[39;49m\u001b[39m'\u001b[39;49m, num_classes)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m loaded\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Load images from a folder\u001b[39;00m\n",
      "\u001b[1;32m/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb Cell 8\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(\u001b[39mcls\u001b[39m, model_path, num_classes):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(num_classes)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m     model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(model_path))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.87.232.58/home/pappol/Scrivania/uni/medical/FemoralHeadSegmentation/SliceClassifierCNN.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/medical/lib/python3.9/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SliceClassifierCNN:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"conv2.weight\", \"conv2.bias\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"conv3.weight\", \"conv3.bias\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"resnet.conv1.weight\", \"resnet.bn1.weight\", \"resnet.bn1.bias\", \"resnet.bn1.running_mean\", \"resnet.bn1.running_var\", \"resnet.bn1.num_batches_tracked\", \"resnet.layer1.0.conv1.weight\", \"resnet.layer1.0.bn1.weight\", \"resnet.layer1.0.bn1.bias\", \"resnet.layer1.0.bn1.running_mean\", \"resnet.layer1.0.bn1.running_var\", \"resnet.layer1.0.bn1.num_batches_tracked\", \"resnet.layer1.0.conv2.weight\", \"resnet.layer1.0.bn2.weight\", \"resnet.layer1.0.bn2.bias\", \"resnet.layer1.0.bn2.running_mean\", \"resnet.layer1.0.bn2.running_var\", \"resnet.layer1.0.bn2.num_batches_tracked\", \"resnet.layer1.1.conv1.weight\", \"resnet.layer1.1.bn1.weight\", \"resnet.layer1.1.bn1.bias\", \"resnet.layer1.1.bn1.running_mean\", \"resnet.layer1.1.bn1.running_var\", \"resnet.layer1.1.bn1.num_batches_tracked\", \"resnet.layer1.1.conv2.weight\", \"resnet.layer1.1.bn2.weight\", \"resnet.layer1.1.bn2.bias\", \"resnet.layer1.1.bn2.running_mean\", \"resnet.layer1.1.bn2.running_var\", \"resnet.layer1.1.bn2.num_batches_tracked\", \"resnet.layer2.0.conv1.weight\", \"resnet.layer2.0.bn1.weight\", \"resnet.layer2.0.bn1.bias\", \"resnet.layer2.0.bn1.running_mean\", \"resnet.layer2.0.bn1.running_var\", \"resnet.layer2.0.bn1.num_batches_tracked\", \"resnet.layer2.0.conv2.weight\", \"resnet.layer2.0.bn2.weight\", \"resnet.layer2.0.bn2.bias\", \"resnet.layer2.0.bn2.running_mean\", \"resnet.layer2.0.bn2.running_var\", \"resnet.layer2.0.bn2.num_batches_tracked\", \"resnet.layer2.0.downsample.0.weight\", \"resnet.layer2.0.downsample.1.weight\", \"resnet.layer2.0.downsample.1.bias\", \"resnet.layer2.0.downsample.1.running_mean\", \"resnet.layer2.0.downsample.1.running_var\", \"resnet.layer2.0.downsample.1.num_batches_tracked\", \"resnet.layer2.1.conv1.weight\", \"resnet.layer2.1.bn1.weight\", \"resnet.layer2.1.bn1.bias\", \"resnet.layer2.1.bn1.running_mean\", \"resnet.layer2.1.bn1.running_var\", \"resnet.layer2.1.bn1.num_batches_tracked\", \"resnet.layer2.1.conv2.weight\", \"resnet.layer2.1.bn2.weight\", \"resnet.layer2.1.bn2.bias\", \"resnet.layer2.1.bn2.running_mean\", \"resnet.layer2.1.bn2.running_var\", \"resnet.layer2.1.bn2.num_batches_tracked\", \"resnet.layer3.0.conv1.weight\", \"resnet.layer3.0.bn1.weight\", \"resnet.layer3.0.bn1.bias\", \"resnet.layer3.0.bn1.running_mean\", \"resnet.layer3.0.bn1.running_var\", \"resnet.layer3.0.bn1.num_batches_tracked\", \"resnet.layer3.0.conv2.weight\", \"resnet.layer3.0.bn2.weight\", \"resnet.layer3.0.bn2.bias\", \"resnet.layer3.0.bn2.running_mean\", \"resnet.layer3.0.bn2.running_var\", \"resnet.layer3.0.bn2.num_batches_tracked\", \"resnet.layer3.0.downsample.0.weight\", \"resnet.layer3.0.downsample.1.weight\", \"resnet.layer3.0.downsample.1.bias\", \"resnet.layer3.0.downsample.1.running_mean\", \"resnet.layer3.0.downsample.1.running_var\", \"resnet.layer3.0.downsample.1.num_batches_tracked\", \"resnet.layer3.1.conv1.weight\", \"resnet.layer3.1.bn1.weight\", \"resnet.layer3.1.bn1.bias\", \"resnet.layer3.1.bn1.running_mean\", \"resnet.layer3.1.bn1.running_var\", \"resnet.layer3.1.bn1.num_batches_tracked\", \"resnet.layer3.1.conv2.weight\", \"resnet.layer3.1.bn2.weight\", \"resnet.layer3.1.bn2.bias\", \"resnet.layer3.1.bn2.running_mean\", \"resnet.layer3.1.bn2.running_var\", \"resnet.layer3.1.bn2.num_batches_tracked\", \"resnet.layer4.0.conv1.weight\", \"resnet.layer4.0.bn1.weight\", \"resnet.layer4.0.bn1.bias\", \"resnet.layer4.0.bn1.running_mean\", \"resnet.layer4.0.bn1.running_var\", \"resnet.layer4.0.bn1.num_batches_tracked\", \"resnet.layer4.0.conv2.weight\", \"resnet.layer4.0.bn2.weight\", \"resnet.layer4.0.bn2.bias\", \"resnet.layer4.0.bn2.running_mean\", \"resnet.layer4.0.bn2.running_var\", \"resnet.layer4.0.bn2.num_batches_tracked\", \"resnet.layer4.0.downsample.0.weight\", \"resnet.layer4.0.downsample.1.weight\", \"resnet.layer4.0.downsample.1.bias\", \"resnet.layer4.0.downsample.1.running_mean\", \"resnet.layer4.0.downsample.1.running_var\", \"resnet.layer4.0.downsample.1.num_batches_tracked\", \"resnet.layer4.1.conv1.weight\", \"resnet.layer4.1.bn1.weight\", \"resnet.layer4.1.bn1.bias\", \"resnet.layer4.1.bn1.running_mean\", \"resnet.layer4.1.bn1.running_var\", \"resnet.layer4.1.bn1.num_batches_tracked\", \"resnet.layer4.1.conv2.weight\", \"resnet.layer4.1.bn2.weight\", \"resnet.layer4.1.bn2.bias\", \"resnet.layer4.1.bn2.running_mean\", \"resnet.layer4.1.bn2.running_var\", \"resnet.layer4.1.bn2.num_batches_tracked\", \"resnet.fc.weight\", \"resnet.fc.bias\". "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "#TEST CLASSIFIER\n",
    "loaded = SliceClassifierCNN.load_model('modelCNN_172058.pth', num_classes)\n",
    "loaded.eval()\n",
    "\n",
    "# Load images from a folder\n",
    "folder_path = \"data/dataset/retrovertedHip/Patient_06/axial\"\n",
    "\n",
    "image_array = []\n",
    "for filename in os.listdir(folder_path):\n",
    "        # Sample every 10 images\n",
    "        if int(filename.split('_')[-2]) % 20 != 0:\n",
    "                continue\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(file_path).convert(\"RGB\").convert(\"L\")\n",
    "        if transform is not None:\n",
    "                image = transform(image)\n",
    "        image_array.append(image)\n",
    "\n",
    "print(f'Number of images: {len(image_array)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the loaded model\n",
    "predictions = []\n",
    "for image in image_array:\n",
    "    image = image.unsqueeze(0)\n",
    "    predictions.append(loaded(image))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the predictions\n",
    "for index, value in enumerate(predictions):\n",
    "    # Get the predicted class with the highest score\n",
    "    pred_class = value.argmax(dim=1, keepdim=True)\n",
    "    if pred_class.item() == 1:\n",
    "        print(f'{index + 1}: Normal')\n",
    "        plt.imshow(image_array[index].squeeze(), cmap='gray')\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|\u001b[31m██████████\u001b[0m| 307/307 [03:22<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.28315032223779524, Accuracy: 0.9077768729641694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|\u001b[31m██████████\u001b[0m| 307/307 [03:31<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.25588731103688966, Accuracy: 0.9084894136807817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|\u001b[31m██████████\u001b[0m| 307/307 [03:36<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.2333352639421772, Accuracy: 0.9113395765472313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|\u001b[31m██████████\u001b[0m| 307/307 [03:23<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.22448269366392873, Accuracy: 0.9120521172638436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|\u001b[31m██████████\u001b[0m| 307/307 [03:30<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.21313238762780198, Accuracy: 0.9164291530944625\n",
      "Training complete!\n",
      "Model saved to modelCNN_172058.pth\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    model = SliceClassifierResNet(num_classes)\n",
    "    #model = SliceClassifierCNN(num_classes)\n",
    "    # Train the model \n",
    "    model.train_model(dataloader, num_epochs, learning_rate)\n",
    "    model.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 100\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "#TEST CLASSIFIER\n",
    "loaded = SliceClassifierResNet.load_model('modelCNN_172058.pth', num_classes)\n",
    "\n",
    "# Load images from a folder\n",
    "folder_path = \"data/dataset/retrovertedHip/Patient_06/axial\"\n",
    "\n",
    "image_array = []\n",
    "for filename in os.listdir(folder_path):\n",
    "        # Sample every 10 images\n",
    "        if int(filename.split('_')[-2]) % 20 != 0:\n",
    "                continue\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(file_path).convert(\"RGB\").convert(\"L\")\n",
    "        if transform is not None:\n",
    "                image = transform(image)\n",
    "        image_array.append(image)\n",
    "\n",
    "print(f'Number of images: {len(image_array)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the loaded model\n",
    "model.cpu()\n",
    "predictions = []\n",
    "for image in image_array:\n",
    "    image = image.unsqueeze(0)\n",
    "    predictions.append(model(image))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the predictions\n",
    "for index, value in enumerate(predictions):\n",
    "    # Get the predicted class with the highest score\n",
    "    pred_class = value.argmax(dim=1, keepdim=True)\n",
    "    if pred_class.item() == 1:\n",
    "        print(f'{index + 1}: Normal')\n",
    "        plt.imshow(image_array[index].squeeze(), cmap='gray')\n",
    "        plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medImg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
